<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>üé§ Talk to Slunt</title>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    
    body {
      font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      color: white;
      min-height: 100vh;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      padding: 20px;
    }
    
    .container {
      max-width: 800px;
      width: 100%;
      background: rgba(0, 0, 0, 0.3);
      backdrop-filter: blur(10px);
      border-radius: 20px;
      padding: 40px;
      box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
    }
    
    h1 {
      font-size: 3em;
      margin-bottom: 10px;
      text-align: center;
      text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.5);
    }
    
    .subtitle {
      text-align: center;
      opacity: 0.8;
      margin-bottom: 40px;
      font-size: 1.2em;
    }
    
    .status {
      text-align: center;
      font-size: 1.5em;
      margin: 30px 0;
      padding: 20px;
      background: rgba(255, 255, 255, 0.1);
      border-radius: 15px;
      border: 2px solid rgba(255, 255, 255, 0.2);
    }
    
    .status.listening {
      background: rgba(76, 175, 80, 0.3);
      border-color: #4CAF50;
      animation: pulse 1.5s ease-in-out infinite;
    }
    
    .status.speaking {
      background: rgba(33, 150, 243, 0.3);
      border-color: #2196F3;
      animation: pulse 1.5s ease-in-out infinite;
    }
    
    @keyframes pulse {
      0%, 100% { transform: scale(1); }
      50% { transform: scale(1.05); }
    }
    
    .controls {
      display: flex;
      gap: 20px;
      justify-content: center;
      margin: 30px 0;
      flex-wrap: wrap;
    }
    
    button {
      padding: 15px 40px;
      font-size: 1.2em;
      border: none;
      border-radius: 50px;
      cursor: pointer;
      transition: all 0.3s ease;
      font-weight: bold;
      box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
    }
    
    button:hover {
      transform: translateY(-2px);
      box-shadow: 0 6px 20px rgba(0, 0, 0, 0.3);
    }
    
    button:active {
      transform: translateY(0);
    }
    
    #startBtn {
      background: linear-gradient(135deg, #4CAF50 0%, #45a049 100%);
      color: white;
    }
    
    #stopBtn {
      background: linear-gradient(135deg, #f44336 0%, #da190b 100%);
      color: white;
    }
    
    #interruptBtn {
      background: linear-gradient(135deg, #FF9800 0%, #F57C00 100%);
      color: white;
    }
    
    #screenShareBtn {
      background: linear-gradient(135deg, #9C27B0 0%, #7B1FA2 100%);
      color: white;
      font-size: 1em;
      padding: 12px 30px;
    }
    
    button:disabled {
      opacity: 0.5;
      cursor: not-allowed;
    }
    
    .transcript {
      margin-top: 30px;
      padding: 20px;
      background: rgba(0, 0, 0, 0.3);
      border-radius: 15px;
      max-height: 400px;
      overflow-y: auto;
    }
    
    .transcript-item {
      margin: 15px 0;
      padding: 15px;
      border-radius: 10px;
      animation: fadeIn 0.3s ease;
    }
    
    @keyframes fadeIn {
      from { opacity: 0; transform: translateY(10px); }
      to { opacity: 1; transform: translateY(0); }
    }
    
    .you {
      background: rgba(76, 175, 80, 0.2);
      border-left: 4px solid #4CAF50;
    }
    
    .slunt {
      background: rgba(33, 150, 243, 0.2);
      border-left: 4px solid #2196F3;
    }
    
    .speaker {
      font-weight: bold;
      margin-bottom: 5px;
    }
    
    .timestamp {
      font-size: 0.8em;
      opacity: 0.6;
      margin-top: 5px;
    }
    
    .error {
      background: rgba(244, 67, 54, 0.3);
      border: 2px solid #f44336;
      padding: 15px;
      border-radius: 10px;
      margin: 20px 0;
      text-align: center;
    }
    
    .info {
      background: rgba(255, 255, 255, 0.1);
      padding: 20px;
      border-radius: 10px;
      margin-top: 30px;
      font-size: 0.9em;
      opacity: 0.8;
    }
    
    .info h3 {
      margin-bottom: 10px;
    }
    
    .info ul {
      margin-left: 20px;
    }
    
    .info li {
      margin: 5px 0;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>üé§ Talk to Slunt</h1>
    <p class="subtitle">Voice conversation with your AI friend</p>
    
    <div id="status" class="status">
      Ready to start
    </div>
    
    <div class="controls">
      <button id="startBtn">üé§ Start Listening</button>
      <button id="stopBtn" disabled>üîá Stop</button>
      <button id="interruptBtn" disabled>‚úã Interrupt Slunt</button>
      <button id="screenShareBtn">üëÅÔ∏è Share Screen (Optional)</button>
    </div>
    
    <div id="screenStatus" class="status" style="display: none; font-size: 1em;">
      üì∫ Screen sharing: <span id="screenStatusText">Off</span>
    </div>
    
    <div id="error" class="error" style="display: none;"></div>
    
    <div class="transcript" id="transcript">
      <p style="text-align: center; opacity: 0.5;">Conversation will appear here...</p>
    </div>
    
    <div class="info">
      <h3>üí° How to use:</h3>
      <ul>
        <li><strong>Click "Start Listening"</strong> to begin voice conversation</li>
        <li><strong>Speak naturally</strong> - Slunt will hear you and respond</li>
        <li><strong>Interrupt anytime</strong> - Click interrupt or just start speaking</li>
        <li><strong>Share Screen (Optional)</strong> - Let Slunt see what you're looking at for context
          <br><em style="opacity: 0.7;">‚Üí You can choose: Entire screen, specific monitor, or just one window</em>
          <br><em style="opacity: 0.7;">‚Üí Great for games on other monitors!</em>
          <br><em style="opacity: 0.7;">‚Üí All screen data auto-deleted after 15 seconds</em>
        </li>
        <li><strong>Works during streaming</strong> - Keep this open while streaming on Twitch</li>
      </ul>
    </div>
  </div>
  
  <!-- Socket.IO client -->
  <script src="/socket.io/socket.io.js"></script>
  <script>
    // Connect to Socket.IO server
    const socket = io();
    
    // UI Elements
    const statusEl = document.getElementById('status');
    const startBtn = document.getElementById('startBtn');
    const stopBtn = document.getElementById('stopBtn');
    const interruptBtn = document.getElementById('interruptBtn');
    const screenShareBtn = document.getElementById('screenShareBtn');
    const screenStatusEl = document.getElementById('screenStatus');
    const screenStatusText = document.getElementById('screenStatusText');
    const transcriptEl = document.getElementById('transcript');
    const errorEl = document.getElementById('error');
    

    // Audio recording for server-side STT
    let mediaRecorder = null;
    let audioChunks = [];
    let isListening = false;
    let isSpeaking = false;
    let currentAudio = null;
    
    // Push-to-talk variables
    let isPushToTalkActive = false;
    let shiftHeld = false;
    let mouseButton4or5Held = false;
    
    // Screen sharing variables
    let screenStream = null;
    let captureInterval = null;
    let isScreenSharing = false;

    // Voice activity detection variables
    let audioContext = null;
    let analyser = null;
    let silenceStart = null;
    let isCurrentlySpeaking = false;
    const SILENCE_THRESHOLD = -45; // dB - adjust if needed
    const SILENCE_DURATION = 250; // ms - FAST auto-stop for snappy responses (reduced from 400ms)
    let lastAudioSentTime = 0;
    let pendingResponseId = null;

    // Transcript auto-clear after silence
    let lastUserSpeechTime = 0;
    let transcriptClearTimer = null;
    const TRANSCRIPT_CLEAR_DELAY = 2000; // Clear after 2 seconds of silence - fast and tight

    function scheduleTranscriptClear() {
      // Cancel any existing timer
      if (transcriptClearTimer) {
        clearTimeout(transcriptClearTimer);
      }
      
      // Set new timer to clear old user messages after silence
      transcriptClearTimer = setTimeout(() => {
        const now = Date.now();
        const timeSinceLastSpeech = now - lastUserSpeechTime;
        
        // Only clear if it's been long enough since last speech
        if (timeSinceLastSpeech >= TRANSCRIPT_CLEAR_DELAY) {
          // Remove all "you" messages from transcript, keep only Slunt's responses
          const transcriptItems = transcriptEl.querySelectorAll('.transcript-item');
          transcriptItems.forEach(item => {
            if (item.classList.contains('you')) {
              item.remove();
            }
          });
          console.log('üóëÔ∏è Cleared old user messages from transcript after silence');
        }
      }, TRANSCRIPT_CLEAR_DELAY);
    }

    // Voice activity detection - monitors audio level
    function checkAudioLevel() {
      if (!analyser || !isPushToTalkActive) return;

      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      analyser.getByteFrequencyData(dataArray);

      // Calculate average volume
      const average = dataArray.reduce((sum, value) => sum + value, 0) / dataArray.length;
      const volumeDb = 20 * Math.log10(average / 255);

      const now = Date.now();

      if (volumeDb > SILENCE_THRESHOLD) {
        // Sound detected
        silenceStart = null;
        isCurrentlySpeaking = true;
      } else if (isCurrentlySpeaking) {
        // Silence detected while speaking
        if (!silenceStart) {
          silenceStart = now;
        } else if (now - silenceStart > SILENCE_DURATION) {
          // Silence lasted long enough - auto-stop
          console.log('üîá AUTO-STOP: Silence detected for ' + SILENCE_DURATION + 'ms');
          statusEl.textContent += ' (auto-stopped after silence)';
          if (mediaRecorder && mediaRecorder.state === 'recording') {
            try {
              mediaRecorder.stop();
            } catch (e) {
              console.log('Error auto-stopping:', e.message);
            }
          }
          isCurrentlySpeaking = false;
          silenceStart = null;
        }
      }

      // Continue monitoring
      if (isPushToTalkActive) {
        requestAnimationFrame(checkAudioLevel);
      }
    }

    // Push-to-talk: Shift + Mouse Button 4 or 5
    function updatePushToTalkState() {
      const wasActive = isPushToTalkActive;
      isPushToTalkActive = shiftHeld && mouseButton4or5Held;

      if (isPushToTalkActive && !wasActive && mediaRecorder && isListening) {
        console.log('üé§ Push-to-talk ACTIVATED - Recording...');
        statusEl.textContent = 'üé§ Recording (speak now, will auto-stop after brief pause)';
        statusEl.className = 'status listening';

        // If user starts speaking again while Slunt is responding, interrupt him
        const now = Date.now();
        if (pendingResponseId && now - lastAudioSentTime < 10000) {
          console.log('üîá User interrupted - canceling pending response');
          socket.emit('voice:interrupt', { responseId: pendingResponseId });
          pendingResponseId = null;
        }
        try {
          audioChunks = [];
          silenceStart = null;
          isCurrentlySpeaking = false;
          mediaRecorder.start();
          // Start monitoring audio for silence detection
          checkAudioLevel();
        } catch (e) {
          console.log('MediaRecorder already started:', e.message);
        }
      } else if (!isPushToTalkActive && wasActive && mediaRecorder && isListening) {
        console.log('üîá Push-to-talk RELEASED - Stopped recording');
        statusEl.textContent = 'Muted (Hold Shift + Mouse Button 4/5 to talk)';
        statusEl.className = 'status';
        isCurrentlySpeaking = false;
        silenceStart = null;
        try {
          if (mediaRecorder.state === 'recording') {
            mediaRecorder.stop();
          }
        } catch (e) {
          console.log('MediaRecorder already stopped:', e.message);
        }
      }
    }

    // Keyboard listeners
    document.addEventListener('keydown', (e) => {
      if (e.key === 'Shift') {
        shiftHeld = true;
        updatePushToTalkState();
      }
    });

    document.addEventListener('keyup', (e) => {
      if (e.key === 'Shift') {
        shiftHeld = false;
        updatePushToTalkState();
      }
    });

    // Mouse button listeners (buttons 3 and 4 are mouse buttons 4 and 5)
    document.addEventListener('mousedown', (e) => {
      if (e.button === 3 || e.button === 4) {
        e.preventDefault();
        mouseButton4or5Held = true;
        updatePushToTalkState();
      }
    });

    document.addEventListener('mouseup', (e) => {
      if (e.button === 3 || e.button === 4) {
        e.preventDefault();
        mouseButton4or5Held = false;
        updatePushToTalkState();
      }
    });

    // Reset on window blur (safety)
    window.addEventListener('blur', () => {
      shiftHeld = false;
      mouseButton4or5Held = false;
      updatePushToTalkState();
    });

    startBtn.addEventListener('click', async () => {
      updateStatus('Requesting microphone...', '');
      try {
        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
          showError('Your browser does not support microphone access (getUserMedia).');
          updateStatus('Microphone not supported', '');
          return;
        }
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

        // Set up audio analyser for voice activity detection
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const source = audioContext.createMediaStreamSource(stream);
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 2048;
        source.connect(analyser);

        updateStatus('Microphone ready - Push-to-talk enabled', '');
        mediaRecorder = new MediaRecorder(stream);
        audioChunks = [];
        mediaRecorder.ondataavailable = (event) => {
          if (event.data.size > 0) {
            audioChunks.push(event.data);
          }
        };
        mediaRecorder.onerror = (event) => {
          console.error('MediaRecorder error:', event.error);
          showError('MediaRecorder error: ' + event.error);
          updateStatus('MediaRecorder error', '');
        };
        mediaRecorder.onstop = async () => {
          if (audioChunks.length === 0) {
            console.log('No audio chunks recorded');
            return;
          }
          const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
          addTranscript('you', '[Voice message sent]');
          updateStatus('Processing...', '');
          // Send audio to server for STT
          const reader = new FileReader();
          reader.onload = function() {
            const arrayBuffer = reader.result;
            socket.emit('voice:audio', arrayBuffer);
            updateStatus('Audio sent to server', '');
          };
          reader.onerror = function(e) {
            showError('Error reading audio: ' + e.message);
            updateStatus('Audio read error', '');
          };
          reader.readAsArrayBuffer(audioBlob);
          audioChunks = []; // Clear for next recording
        };
        isListening = true;
        updateStatus('Muted (Hold Shift + Mouse Button 4/5 to talk)', '');
        startBtn.disabled = true;
        stopBtn.disabled = false;
        interruptBtn.disabled = false;
        clearError();
        transcriptEl.innerHTML = '<p style="text-align: center; opacity: 0.5;">Push-to-talk ready...</p>';
        console.log('üé§ Push-to-talk mode enabled. Hold Shift + Mouse Button 4 or 5 to speak.');
      } catch (error) {
        console.error('Error starting microphone:', error);
        showError('Error starting microphone: ' + error.message);
        updateStatus('Microphone error', '');
      }
    });

    stopBtn.addEventListener('click', () => {
      if (mediaRecorder && isListening) {
        try {
          if (mediaRecorder.state === 'recording') {
            mediaRecorder.stop();
          }
        } catch (e) {}
        isListening = false;
        isPushToTalkActive = false;
        shiftHeld = false;
        mouseButton4or5Held = false;
        updateStatus('Stopped', '');
        startBtn.disabled = false;
        stopBtn.disabled = true;
        interruptBtn.disabled = true;
      }
    });

    interruptBtn.addEventListener('click', () => {
      if (isSpeaking) {
        interruptSpeech();
      }
    });

    function interruptSpeech() {
      if (currentAudio) {
        currentAudio.pause();
        currentAudio = null;
      }
      isSpeaking = false;
      socket.emit('voice:interrupt');
      updateStatus('Interrupted - Listening...', 'listening');
      addTranscript('system', '‚è∏Ô∏è Speech interrupted');
    }
    
    // Screen sharing functionality
    screenShareBtn.addEventListener('click', async () => {
      if (isScreenSharing) {
        stopScreenSharing();
      } else {
        await startScreenSharing();
      }
    });
    
    async function startScreenSharing() {
      try {
        if (!navigator.mediaDevices || !navigator.mediaDevices.getDisplayMedia) {
          showError('Screen sharing not supported in this browser');
          return;
        }
        
        // Request screen sharing - browser will show picker for screen/monitor/window
        screenStream = await navigator.mediaDevices.getDisplayMedia({
          video: { 
            width: 1920, 
            height: 1080,
            frameRate: 1 // Low frame rate for efficiency
          },
          audio: false,
          // Prefer entire screen/monitor for games, but user can still choose window
          preferCurrentTab: false
        });
        
        isScreenSharing = true;
        screenShareBtn.textContent = 'üõë Stop Sharing Screen';
        screenShareBtn.style.background = 'linear-gradient(135deg, #f44336 0%, #da190b 100%)';
        screenStatusEl.style.display = 'block';
        screenStatusText.textContent = 'Active';
        screenStatusText.style.color = '#4CAF50';
        
        // Handle stream ending (user stops sharing)
        screenStream.getVideoTracks()[0].addEventListener('ended', () => {
          stopScreenSharing();
        });
        
        // Capture and send frames every 5 seconds
        captureInterval = setInterval(() => {
          if (isScreenSharing) {
            captureAndSendFrame();
          }
        }, 5000);
        
        // Send initial frame
        captureAndSendFrame();
        
        console.log('Screen sharing started');
      } catch (error) {
        console.error('Screen sharing error:', error);
        if (error.name === 'NotAllowedError') {
          showError('Screen sharing permission denied');
        } else {
          showError('Screen sharing error: ' + error.message);
        }
      }
    }
    
    function stopScreenSharing() {
      if (screenStream) {
        screenStream.getTracks().forEach(track => track.stop());
        screenStream = null;
      }
      
      if (captureInterval) {
        clearInterval(captureInterval);
        captureInterval = null;
      }
      
      isScreenSharing = false;
      screenShareBtn.textContent = 'üëÅÔ∏è Share Screen (Optional)';
      screenShareBtn.style.background = '';
      screenStatusEl.style.display = 'none';
      screenStatusText.textContent = 'Off';
      
      console.log('Screen sharing stopped');
    }
    
    async function captureAndSendFrame() {
      if (!screenStream || !isScreenSharing) return;
      
      try {
        // Create video element
        const video = document.createElement('video');
        video.srcObject = screenStream;
        video.muted = true;
        await video.play();
        
        // Wait for video to be ready
        await new Promise(resolve => {
          if (video.readyState >= 2) resolve();
          else video.addEventListener('loadedmetadata', resolve, { once: true });
        });
        
        // Create canvas and capture frame
        const canvas = document.createElement('canvas');
        canvas.width = Math.min(video.videoWidth, 1920);
        canvas.height = Math.min(video.videoHeight, 1080);
        const ctx = canvas.getContext('2d');
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        
        // Convert to JPEG blob (compressed)
        canvas.toBlob((blob) => {
          if (blob) {
            const reader = new FileReader();
            reader.onloadend = () => {
              // Send base64 image to server
              socket.emit('voice:screenFrame', {
                data: reader.result,
                timestamp: Date.now(),
                description: 'User screen content',
                deleteAfterProcessing: true // Tell server to delete after analysis
              });
              console.log('Screen frame sent to server (will be auto-deleted)');
            };
            reader.readAsDataURL(blob);
          }
        }, 'image/jpeg', 0.7); // 70% quality for better detail (games/action)
        
        // Clean up video
        video.pause();
        video.srcObject = null;
      } catch (error) {
        console.error('Frame capture error:', error);
      }
    }
    
    // Socket events
    socket.on('voice:response', async (data) => {
      console.log('Slunt response:', data);
      pendingResponseId = null; // Clear pending response - we got it!
      addTranscript('slunt', data.text, data.timing);
      // Play audio if available
      if (data.audioUrl) {
        await playAudio(data.audioUrl);
      } else if ('speechSynthesis' in window) {
        // Fallback to browser speech synthesis if no server audio
        try {
          const utter = new SpeechSynthesisUtterance(data.text || '');
          utter.rate = 1.0;
          utter.pitch = 1.0;
          utter.onstart = () => updateStatus('Slunt is speaking...', 'speaking');
          utter.onend = () => {
            isSpeaking = false;
            if (isListening) updateStatus('Listening...', 'listening');
          };
          window.speechSynthesis.cancel();
          window.speechSynthesis.speak(utter);
        } catch (e) {
          console.warn('Speech synthesis error:', e);
          updateStatus('Listening...', 'listening');
        }
      } else {
        updateStatus('Listening...', 'listening');
      }
    });

    // Display transcript from server STT
    socket.on('voice:transcript', (data) => {
      if (data && data.text) {
        addTranscript('you', data.text);
        updateStatus('Transcript received', '');
        
        // Track when user last spoke
        lastUserSpeechTime = Date.now();
        
        // Schedule transcript clearing after silence
        scheduleTranscriptClear();
        
        // Send recognized text for Slunt to respond
        lastAudioSentTime = Date.now();
        pendingResponseId = Date.now(); // Use timestamp as simple ID

        // Use streaming for faster perceived response time
        const useStreaming = true; // Enable streaming mode
        if (useStreaming) {
          socket.emit('voice:speech:stream', { text: data.text, responseId: pendingResponseId });
        } else {
          socket.emit('voice:speech', { text: data.text, responseId: pendingResponseId });
        }
      } else {
        showError('No transcript received');
      }
    });
    
    socket.on('voice:speaking:started', () => {
      isSpeaking = true;
      updateStatus('Slunt is speaking...', 'speaking');
    });
    
    socket.on('voice:speaking:finished', () => {
      isSpeaking = false;
      if (isListening) {
        updateStatus('Listening...', 'listening');
      }
    });
    
    socket.on('voice:error', (data) => {
      console.error('Voice error:', data);
      showError(data.message || 'Voice system error');
    });

    // Streaming audio handlers
    let streamingAudioQueue = [];
    let streamingResponseId = null;
    let streamingText = '';

    socket.on('voice:stream:start', (data) => {
      console.log('üéôÔ∏è Stream started:', data.text);
      streamingResponseId = data.responseId;
      streamingText = data.text;
      streamingAudioQueue = [];
      isSpeaking = true;
      updateStatus('Slunt is speaking (streaming)...', 'speaking');
    });

    socket.on('voice:stream:chunk', async (data) => {
      if (data.responseId !== streamingResponseId) {
        console.warn('‚ö†Ô∏è Received chunk for different response, ignoring');
        return;
      }

      console.log(`üì¶ Received chunk ${data.chunkIdx}: ${data.audioUrl}`);

      // Queue the chunk
      streamingAudioQueue.push({
        idx: data.chunkIdx,
        audioUrl: data.audioUrl,
        timestamp: data.timestamp
      });

      // Sort by index to ensure correct order
      streamingAudioQueue.sort((a, b) => a.idx - b.idx);

      // Start playing if not already playing
      if (!currentAudio && streamingAudioQueue.length > 0) {
        playNextStreamChunk();
      }
    });

    socket.on('voice:stream:end', (data) => {
      console.log(`‚úÖ Stream complete: ${data.totalChunks} chunks`);

      // Add to transcript with timing
      if (streamingText) {
        addTranscript('slunt', streamingText, data.timing);
      }

      // Reset streaming state
      streamingResponseId = null;
      streamingText = '';
    });

    async function playNextStreamChunk() {
      if (streamingAudioQueue.length === 0) {
        // No more chunks to play
        if (streamingResponseId === null) {
          // Stream is complete
          isSpeaking = false;
          currentAudio = null;
          if (isListening) {
            updateStatus('Listening...', 'listening');
          }
        }
        return;
      }

      const chunk = streamingAudioQueue.shift();
      console.log(`‚ñ∂Ô∏è Playing chunk ${chunk.idx}`);

      try {
        currentAudio = new Audio(chunk.audioUrl);
        currentAudio.playbackRate = 1.5; // Hoff talks quick

        currentAudio.onended = () => {
          currentAudio = null;
          // Play next chunk
          playNextStreamChunk();
        };

        currentAudio.onerror = (error) => {
          console.error('Streaming chunk playback error:', error);
          currentAudio = null;
          // Try next chunk
          playNextStreamChunk();
        };

        await currentAudio.play();
      } catch (error) {
        console.error('Error playing streaming chunk:', error);
        currentAudio = null;
        // Try next chunk
        playNextStreamChunk();
      }
    }

    // Detect when user starts speaking (for interruption)
    if (typeof recognition !== 'undefined' && recognition) {
      const orig = recognition.onspeechstart;
      recognition.onspeechstart = () => {
        if (isSpeaking) interruptSpeech();
        if (typeof orig === 'function') orig();
      };
    }
    
    async function playAudio(audioUrl) {
      try {
        currentAudio = new Audio(audioUrl);
        
        // Play Monster voice 30% faster
        currentAudio.playbackRate = 1.5; // 50% faster - Hoff talks quick
        
        currentAudio.onended = () => {
          isSpeaking = false;
          currentAudio = null;
          if (isListening) {
            updateStatus('Listening...', 'listening');
          }
        };
        
        currentAudio.onerror = (error) => {
          console.error('Audio playback error:', error);
          showError('Error playing audio');
          isSpeaking = false;
          currentAudio = null;
        };
        
        await currentAudio.play();
      } catch (error) {
        console.error('Error playing audio:', error);
        showError('Error playing audio: ' + error.message);
      }
    }
    
    function updateStatus(text, className = '') {
      statusEl.textContent = text;
      statusEl.className = 'status ' + className;
    }
    
    function addTranscript(speaker, text, timing) {
      const item = document.createElement('div');
      item.className = `transcript-item ${speaker}`;
      
      const speakerLabel = speaker === 'you' ? 'You' : speaker === 'slunt' ? 'Slunt' : 'System';
      const timestamp = new Date().toLocaleTimeString();
      
      // Add timing info if available
      let timingInfo = '';
      if (timing && timing.total) {
        const aiTime = timing.ai || 0;
        const ttsTime = timing.tts || 0;
        timingInfo = `<div class="timing" style="font-size: 10px; color: #666; margin-top: 2px;">‚è±Ô∏è ${timing.total}ms (AI: ${aiTime}ms, TTS: ${ttsTime}ms)</div>`;
      }
      
      item.innerHTML = `
        <div class="speaker">${speakerLabel}</div>
        <div>${text}</div>
        <div class="timestamp">${timestamp}</div>
        ${timingInfo}
      `;
      
      // Remove placeholder text if exists
      if (transcriptEl.querySelector('p')) {
        transcriptEl.innerHTML = '';
      }
      
      transcriptEl.appendChild(item);
      transcriptEl.scrollTop = transcriptEl.scrollHeight;
    }
    
    function showError(message) {
      errorEl.textContent = message;
      errorEl.style.display = 'block';
    }
    
    function clearError() {
      errorEl.style.display = 'none';
    }
    
    // Check if voice is enabled on server
    socket.emit('voice:status', (status) => {
      if (!status.enabled) {
        showError('Voice system not enabled on server. Check configuration.');
      }
    });
  </script>
</body>
</html>
