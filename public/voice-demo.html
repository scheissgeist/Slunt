<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
  <meta http-equiv="Pragma" content="no-cache">
  <meta http-equiv="Expires" content="0">
  <title>Slunt Voice Demo v2.0 - FIXED</title>
  <style>
    body { font-family: Arial, sans-serif; background: #222; color: #fff; text-align: center; }
    .container { margin: 40px auto; max-width: 500px; background: #333; border-radius: 12px; padding: 32px; }
    button { font-size: 1.2em; padding: 12px 32px; border-radius: 8px; border: none; background: #4CAF50; color: #fff; cursor: pointer; margin: 12px; }
    button:disabled { background: #888; }
    .transcript, .reply { margin: 24px 0; padding: 16px; background: #444; border-radius: 8px; min-height: 40px; }
    .status-bar {
      margin: 20px 0;
      padding: 16px;
      background: rgba(0,0,0,0.3);
      border-radius: 8px;
      text-align: left;
      border-left: 4px solid #888;
    }
    .status-bar.ready { border-left-color: #4CAF50; }
    .status-bar.loading { border-left-color: #FFC107; }
    .status-bar.error { border-left-color: #f44336; }
    .status-label { font-size: 0.9em; color: #aaa; margin-bottom: 4px; }
    .status-value { font-size: 1.1em; font-weight: bold; }
    .voice-selector { margin: 16px 0; }
    .voice-selector select {
      padding: 8px 16px;
      font-size: 1em;
      border-radius: 6px;
      background: #555;
      color: #fff;
      border: 2px solid #666;
      cursor: pointer;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>üé§ Talk to Slunt</h1>
    
    <!-- Voice Status Display -->
    <div class="status-bar" id="voiceStatus">
      <div class="status-label">Voice System Status</div>
      <div class="status-value" id="statusText">Checking...</div>
      <div style="margin-top: 8px; font-size: 0.85em; color: #999;" id="statusDetails"></div>
    </div>

    <!-- Voice Selector (future) -->
    <div class="voice-selector" style="display:none;" id="voiceSelector">
      <select id="voiceSelect">
        <option value="xtts">XTTS - Trained Hoff (Local)</option>
        <option value="elevenlabs">ElevenLabs - Hoff (API)</option>
        <option value="browser">Browser TTS (Fallback)</option>
      </select>
    </div>
    
    <button id="startBtn">Start Listening</button>
    <button id="stopBtn" disabled>Stop</button>
    <button id="clearMemoryBtn" style="background: #f44336;">Clear Memory</button>
    <button id="screenShareBtn" style="background: #9C27B0;">üëÅÔ∏è Share Screen (Optional)</button>
    <button id="testScreenBtn" style="background: #FFA500;">üîç Test Screen Vision</button>

    <div id="screenStatus" style="display: none; margin: 12px 0; padding: 12px; background: rgba(0,0,0,0.3); border-radius: 8px; font-size: 0.9em;">
      üì∫ Screen sharing: <span id="screenStatusText" style="color: #4CAF50;">Off</span>
    </div>

    <div id="testResults" style="display: none; margin: 12px 0; padding: 12px; background: rgba(255,165,0,0.2); border-radius: 8px; border-left: 4px solid #FFA500; font-size: 0.85em; white-space: pre-wrap;"></div>
    
    <div class="transcript" id="transcript">Transcript will appear here...</div>
    <div class="reply" id="reply">Slunt's reply will appear here...</div>
    <audio id="audio" controls style="display:none;"></audio>
  </div>
  <script>
    let isListening = false;
    const startBtn = document.getElementById('startBtn');
    const stopBtn = document.getElementById('stopBtn');
    const clearMemoryBtn = document.getElementById('clearMemoryBtn');
    const screenShareBtn = document.getElementById('screenShareBtn');
    const testScreenBtn = document.getElementById('testScreenBtn');
    const testResultsEl = document.getElementById('testResults');
    const screenStatusEl = document.getElementById('screenStatus');
    const screenStatusText = document.getElementById('screenStatusText');
    const transcriptEl = document.getElementById('transcript');
    const replyEl = document.getElementById('reply');
    const audioEl = document.getElementById('audio');
    let synthUtterance = null;
    
    // Screen sharing variables
    let screenStream = null;
    let captureInterval = null;
    let isScreenSharing = false;

    // Voice status monitoring
    async function updateVoiceStatus() {
      const statusBar = document.getElementById('voiceStatus');
      const statusText = document.getElementById('statusText');
      const statusDetails = document.getElementById('statusDetails');
      
      try {
        const response = await fetch('/api/voice/status');
        const status = await response.json();
        
        if (status.available && status.modelStatus === 'healthy') {
          statusBar.className = 'status-bar ready';
          statusText.textContent = `‚úÖ ${status.provider.toUpperCase()} - Ready`;
          
          let details = '';
          if (status.modelInfo.model) {
            details += `Model: ${status.modelInfo.model}`;
          }
          if (status.modelInfo.device) {
            details += ` | Device: ${status.modelInfo.device}`;
          }
          if (status.modelInfo.embeddingsCached !== undefined) {
            details += ` | Precomputed: ${status.modelInfo.embeddingsCached ? 'Yes' : 'No'}`;
          }
          statusDetails.textContent = details;
          
        } else if (status.modelStatus === 'loading') {
          statusBar.className = 'status-bar loading';
          statusText.textContent = `‚è≥ ${status.provider.toUpperCase()} - Loading Model...`;
          statusDetails.textContent = 'This may take 2-4 minutes for large models';
          
        } else {
          statusBar.className = 'status-bar error';
          statusText.textContent = `‚ö†Ô∏è ${status.provider.toUpperCase()} - ${status.modelStatus}`;
          statusDetails.textContent = status.error || 'Voice server offline or not configured';
        }
      } catch (err) {
        statusBar.className = 'status-bar error';
        statusText.textContent = '‚ùå Cannot reach server';
        statusDetails.textContent = err.message;
      }
    }

    // Update status every 5 seconds
    updateVoiceStatus();
    setInterval(updateVoiceStatus, 5000);

    // Clear memory button handler
    clearMemoryBtn.addEventListener('click', async () => {
      try {
        const response = await fetch('/api/voice/clear-memory', { method: 'POST' });
        const data = await response.json();
        replyEl.textContent = 'üß† Memory cleared! Starting fresh conversation.';
        transcriptEl.textContent = 'Ready for a new conversation...';
        console.log('Voice memory cleared');
      } catch (err) {
        console.error('Failed to clear memory:', err);
        replyEl.textContent = '‚ùå Failed to clear memory';
      }
    });

    // Use browser SpeechRecognition for demo (continuous with pause detection)
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
  let recognition = null;
  let listenContinuously = false;
  let preferredVoice = null;
    if (SpeechRecognition) {
      recognition = new SpeechRecognition();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'en-US';
      let finalTranscript = '';
      let lastSpeechTime = 0;
      const PAUSE_MS = 700; // Quick response - natural conversation timing
      const MIN_CONFIDENCE = 0.3; // LOWERED: Accept more speech (was 0.6, too strict)
      let pauseTimer = null; // ADDED: Track single timer to prevent duplicates
      let isSending = false; // ADDED: Prevent multiple sends of same message
      let lastSentMessage = ''; // ADDED: Dedup identical messages
      let lastSentTime = 0; // ADDED: Prevent rapid-fire sends

      // Choose a high-quality system voice if we use browser TTS fallback
      function pickPreferredVoice() {
        const voices = window.speechSynthesis ? window.speechSynthesis.getVoices() : [];
        // Prefer Microsoft neural/natural voices on Windows/Edge
        const prefer = [
          'Microsoft Aria Online (Natural) - English (United States)',
          'Microsoft Guy Online (Natural) - English (United States)'
        ];
        let best = null;
        for (const name of prefer) {
          best = voices.find(v => v.name === name);
          if (best) return best;
        }
        // Otherwise pick first en-US voice that contains Microsoft or Natural/Neural
        best = voices.find(v => /en-US/i.test(v.lang) && /Microsoft|Natural|Neural/i.test(v.name));
        if (best) return best;
        // Fallback to any en-US voice
        return voices.find(v => /en-US/i.test(v.lang)) || voices[0] || null;
      }

      function ensurePreferredVoice() {
        if (!preferredVoice && window.speechSynthesis) {
          preferredVoice = pickPreferredVoice();
        }
      }
      if (window.speechSynthesis) {
        // voices may load asynchronously
        window.speechSynthesis.onvoiceschanged = () => {
          preferredVoice = pickPreferredVoice();
        };
        ensurePreferredVoice();
      }

      // DISABLED: Don't interrupt Slunt while he's speaking
      // Only interrupt if user speaks WHILE he's still playing audio
      // recognition.onstart = () => {
      //   try {
      //     if (!audioEl.paused) {
      //       audioEl.pause();
      //       audioEl.currentTime = 0;
      //     }
      //   } catch (e) {}
      //   try {
      //     if (window.speechSynthesis) {
      //       window.speechSynthesis.cancel();
      //     }
      //   } catch (e) {}
      // };

      recognition.onresult = async (event) => {
        let interim = '';
        // ACCUMULATE ALL SPEECH: Don't reset on interim results
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const res = event.results[i];
          const text = res[0].transcript;
          const conf = res[0].confidence;
          
          // DEBUG: Show what we're getting
          if (res.isFinal) {
            console.log(`[Speech] FINAL: "${text}" (confidence: ${conf.toFixed(2)})`);
          }
          
          // ACCEPT EVERYTHING - no confidence filter
          if (res.isFinal) {
            finalTranscript += text + ' ';
            lastSpeechTime = Date.now();
            console.log(`[Speech] ‚úÖ Accepted: "${text}"`);
          } else if (!res.isFinal) {
            interim += text;
            // Update last speech time even for interim (user still talking)
            lastSpeechTime = Date.now();
          }
        }
        
        // Show accumulated + interim text (full thought)
        const fullText = (finalTranscript + ' ' + interim).trim();
        transcriptEl.textContent = fullText || 'Listening...';

        // FIXED: Clear previous timer and set ONE timer
        // This prevents multiple sends of the same message
        if (pauseTimer) {
          clearTimeout(pauseTimer);
          pauseTimer = null;
        }

        // If we've had a pause after some speech, send EVERYTHING accumulated
        pauseTimer = setTimeout(async () => {
          pauseTimer = null; // Clear timer reference
          
          const text = finalTranscript.trim();
          
          // Skip if already sending, no text, or duplicate message
          if (isSending || !text) {
            return;
          }
          
          // Dedup: Don't send if it's the same message within 3 seconds
          const now = Date.now();
          if (text === lastSentMessage && (now - lastSentTime) < 3000) {
            console.log(`[Speech] ‚è≠Ô∏è Skipping duplicate: "${text}"`);
            return;
          }
          
          console.log(`[Speech] üöÄ Sending to Slunt: "${text}"`);
          
          // Lock to prevent duplicate sends
          isSending = true;
          lastSentMessage = text;
          lastSentTime = now;
          finalTranscript = ''; // Reset for next thought
          transcriptEl.textContent = text;
          
          // ONLY interrupt if Slunt is CURRENTLY playing audio
          try {
            if (!audioEl.paused) {
              console.log('[Speech] ‚ö†Ô∏è Interrupting Slunt (user spoke while he was talking)');
              audioEl.pause();
              audioEl.currentTime = 0;
            }
          } catch (e) {}

          try {
            // Send voice request to server
            const res = await fetch('/api/voice', {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({ text })
            });
            const data = await res.json();

            console.log(`‚úÖ [Voice] Response received: "${data.reply?.substring(0, 50)}..."`);

            // Check if Slunt referenced screen context
            if (data.reply?.includes('screen') || data.reply?.includes('see') || data.reply?.includes('watching')) {
              console.log('üëÅÔ∏è [Voice] Slunt may be referencing what he sees!');
            }
            replyEl.textContent = data.reply || '[No reply]';
            if (data.audioUrl) {
              audioEl.src = data.audioUrl;
              audioEl.style.display = 'block';
              audioEl.play();
            } else if (window.speechSynthesis && data.reply) {
              // Fallback to browser TTS if server could not provide audio
              try {
                window.speechSynthesis.cancel();
                synthUtterance = new SpeechSynthesisUtterance(data.reply);
                ensurePreferredVoice();
                if (preferredVoice) synthUtterance.voice = preferredVoice;
                synthUtterance.rate = 1.0;
                synthUtterance.pitch = 1.0;
                window.speechSynthesis.speak(synthUtterance);
              } catch (e) {
                console.warn('Speech synthesis failed:', e);
              }
            }
          } catch (err) {
            console.error('[Speech] ‚ùå Send failed:', err);
            replyEl.textContent = '‚ùå Failed to send message';
          } finally {
            // Unlock after response or error
            isSending = false;
          }
        }, PAUSE_MS + 10); // Minimal extra delay for snappy responses
      };
      recognition.onerror = (e) => {
        console.error('[Speech] Recognition error:', e.error);
        if (e.error !== 'no-speech' && e.error !== 'aborted') {
          transcriptEl.textContent = 'Error: ' + e.error;
        }
      };
      recognition.onend = () => {
        if (listenContinuously) {
          try {
            recognition.start();
          } catch (e) {
            console.log('[Speech] Recognition restart skipped:', e.message);
          }
        }
      };
    }

    startBtn.onclick = () => {
      if (recognition && !isListening) {
        listenContinuously = true;
        isListening = true;
        startBtn.disabled = true;
        stopBtn.disabled = false;
        startBtn.textContent = 'üé§ Listening';
        transcriptEl.textContent = 'Listening...';
        transcriptEl.style.borderLeft = '4px solid #4CAF50';
        try {
          recognition.start();
        } catch (e) {
          console.log('[Speech] Recognition start failed:', e.message);
        }
      }
    };
    stopBtn.onclick = () => {
      if (recognition && isListening) {
        listenContinuously = false;
        try {
          recognition.stop();
        } catch (e) {}
        isListening = false;
        startBtn.disabled = false;
        stopBtn.disabled = true;
        startBtn.textContent = 'Start Listening';
        transcriptEl.style.borderLeft = '';
        transcriptEl.textContent = 'Stopped';
      }
    };
    
    // Screen sharing functionality
    screenShareBtn.onclick = async () => {
      if (isScreenSharing) {
        stopScreenSharing();
      } else {
        await startScreenSharing();
      }
    };
    
    async function startScreenSharing() {
      try {
        if (!navigator.mediaDevices || !navigator.mediaDevices.getDisplayMedia) {
          alert('Screen sharing not supported in this browser');
          return;
        }
        
        // Initialize Socket.IO for screen sharing (lazy load)
        if (!window.socket) {
          await loadSocketIO();
        }
        
        // Request screen sharing - browser will show picker
        screenStream = await navigator.mediaDevices.getDisplayMedia({
          video: { 
            width: 1920, 
            height: 1080,
            frameRate: 1
          },
          audio: false,
          preferCurrentTab: false
        });
        
        isScreenSharing = true;
        screenShareBtn.textContent = 'üõë Stop Sharing Screen';
        screenShareBtn.style.background = '#f44336';
        screenStatusEl.style.display = 'block';
        screenStatusText.textContent = 'Active';
        screenStatusText.style.color = '#4CAF50';
        
        // Handle stream ending
        screenStream.getVideoTracks()[0].addEventListener('ended', () => {
          stopScreenSharing();
        });
        
        // Capture and send frames every 3 seconds (faster updates for gameplay)
        captureInterval = setInterval(() => {
          if (isScreenSharing) {
            captureAndSendFrame();
          }
        }, 3000);

        // Send initial frame immediately
        console.log('üì∏ [Client] Screen sharing started - sending first frame...');
        captureAndSendFrame();
        
        console.log('Screen sharing started');
      } catch (error) {
        console.error('Screen sharing error:', error);
        if (error.name === 'NotAllowedError') {
          alert('Screen sharing permission denied');
        } else {
          alert('Screen sharing error: ' + error.message);
        }
      }
    }
    
    function stopScreenSharing() {
      if (screenStream) {
        screenStream.getTracks().forEach(track => track.stop());
        screenStream = null;
      }
      
      if (captureInterval) {
        clearInterval(captureInterval);
        captureInterval = null;
      }
      
      isScreenSharing = false;
      screenShareBtn.textContent = 'üëÅÔ∏è Share Screen (Optional)';
      screenShareBtn.style.background = '#9C27B0';
      screenStatusEl.style.display = 'none';
      screenStatusText.textContent = 'Off';
      
      console.log('Screen sharing stopped');
    }
    
    async function captureAndSendFrame() {
      if (!screenStream || !isScreenSharing || !window.socket) return;
      
      try {
        const video = document.createElement('video');
        video.srcObject = screenStream;
        video.muted = true;
        await video.play();
        
        await new Promise(resolve => {
          if (video.readyState >= 2) resolve();
          else video.addEventListener('loadedmetadata', resolve, { once: true });
        });
        
        const canvas = document.createElement('canvas');
        canvas.width = Math.min(video.videoWidth, 1920);
        canvas.height = Math.min(video.videoHeight, 1080);
        const ctx = canvas.getContext('2d');
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        
        canvas.toBlob((blob) => {
          if (blob) {
            const reader = new FileReader();
            reader.onloadend = () => {
              window.socket.emit('voice:screenFrame', {
                data: reader.result,
                timestamp: Date.now(),
                description: 'User screen content',
                deleteAfterProcessing: true
              });
              console.log('üì∏ [Client] Screen frame sent to server for vision analysis');
            };
            reader.readAsDataURL(blob);
          }
        }, 'image/jpeg', 0.7);
        
        video.pause();
        video.srcObject = null;
      } catch (error) {
        console.error('Frame capture error:', error);
      }
    }
    
    // Lazy load Socket.IO only when needed for screen sharing
    function loadSocketIO() {
      return new Promise((resolve, reject) => {
        if (window.io) {
          if (!window.socket) {
            window.socket = io();
          }
          resolve();
          return;
        }
        
        const script = document.createElement('script');
        script.src = '/socket.io/socket.io.js';
        script.onload = () => {
          window.socket = io();
          console.log('Socket.IO loaded for screen sharing');
          resolve();
        };
        script.onerror = () => reject(new Error('Failed to load Socket.IO'));
        document.head.appendChild(script);
      });
    }
  </script>
</body>
</html>
